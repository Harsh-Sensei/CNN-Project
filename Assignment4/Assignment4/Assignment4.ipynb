{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enormous-digit",
   "metadata": {},
   "source": [
    "# Assignment 4 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-examination",
   "metadata": {},
   "source": [
    "In this assignment, you will design an artificial neural network for a binary classification task. We will use Keras for implementing the neural network in this assignment. You can also use numpy, pandas and/or scikit-learn wherever you find them useful. You'll also need matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-gallery",
   "metadata": {},
   "source": [
    "#### Import the required libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cathedral-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "from math import *\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-conditions",
   "metadata": {},
   "source": [
    "Load the dataset from the file named 'data.csv' . You'll find 7200 datapoints in this file. The first 6 columns are the features (X), while the last column has a binary label (Y) for each feature vector. After loading the dataset, divide it into a training set and a test set (cross-validation set to be more accurate). You can have 70% datapoints in the training set and 30% in the test set. An 80-20 split is also acceptable.\n",
    "\n",
    "Normalize your training set using mean and variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "narrow-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd=pd.read_csv(\"data.csv\",header=None)\n",
    "data_np=np.loadtxt(\"data.csv\",delimiter=',')\n",
    "\n",
    "training=data_np[:int(0.7*len(data_np)),:]\n",
    "test=data_np[int(0.7*len(data_np)):,:]\n",
    "\n",
    "trainingx=training[:,:6]\n",
    "trainingy=training[:,6:]\n",
    "\n",
    "\n",
    "testx=test[:,:6]\n",
    "testy=test[:,6:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "equivalent-middle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.973799150320701e-14\n",
      "-8.748557434046234e-14\n",
      "7.993605777301127e-14\n",
      "-4.973799150320701e-14\n",
      "1.509903313490213e-13\n",
      "4.796163466380676e-14\n",
      "[[ 0.63856731 -0.15430186 -0.13864884 -0.0245122  -0.06581879  0.0037502 ]\n",
      " [ 0.10592967 -0.15468232 -0.12234361  0.00048916 -0.010381   -0.03755639]\n",
      " [ 0.35594326 -0.15288874 -0.1288657  -0.04407848 -0.01255503 -0.07016686]\n",
      " ...\n",
      " [ 0.20701165 -0.15320112 -0.1302224  -0.02064793 -0.0483075  -0.02064793]\n",
      " [ 0.31339463 -0.15128623 -0.12809474 -0.06107346 -0.04617984 -0.06320112]\n",
      " [ 0.21764995 -0.15460537 -0.13128623 -0.01639261 -0.05788197 -0.00256282]]\n",
      "[[0.018721   0.00038549 0.00993678 0.01468734 0.01147148 0.01756805]\n",
      " [0.00615485 0.00016062 0.01987356 0.01750242 0.01860619 0.01299554]\n",
      " [0.01205324 0.00122071 0.01589885 0.01248424 0.01832639 0.00938567]\n",
      " ...\n",
      " [0.00871937 0.00089947 0.01523639 0.01542171 0.01398961 0.01516147]\n",
      " [0.01128389 0.00205593 0.0165613  0.01077072 0.01426941 0.0103483 ]\n",
      " [0.00897582 0.0000514  0.01457394 0.01591129 0.01273055 0.01720706]]\n"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    m=len(data)\n",
    "    n=len(data[0])\n",
    "    data=(data.flatten()).reshape(n,m)\n",
    "    for i in range(6):\n",
    "        mean=np.mean(data[i])\n",
    "        max=np.max(data[i])\n",
    "        min=np.min(data[i])\n",
    "        #print(mean,\" \",max,\" \",min)\n",
    "        data[i]=(data[i]-(np.ones(len(data[i])))*mean)/(max-min)\n",
    "        print(np.sum(data[i]))\n",
    "    \n",
    "    return (data.flatten()).reshape(m,n)\n",
    "\n",
    "normalized_trainingx=normalize(trainingx)\n",
    "print(normalized_trainingx)\n",
    "\n",
    "A=sklearn.preprocessing.normalize(trainingx, axis=0)\n",
    "B=sklearn.preprocessing.normalize(testx, axis=0)\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-bones",
   "metadata": {},
   "source": [
    "Find the number of positive and negative samples in the training set /test set /the whole dataset. You'll use this result while evaluating your neural network model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "purple-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5040\n",
      "2160\n",
      "370\n",
      "164\n",
      "534\n"
     ]
    }
   ],
   "source": [
    "cnt_trn_1=0\n",
    "cnt_tst_1=0\n",
    "cnt_data_1=0\n",
    "print(len(trainingy))\n",
    "print(len(testx))\n",
    "for i in trainingy:\n",
    "    if int(i)==1:\n",
    "        cnt_trn_1+=1\n",
    "        \n",
    "print(cnt_trn_1)\n",
    "\n",
    "for i in testy:\n",
    "    if int(i)==1:\n",
    "        cnt_tst_1+=1\n",
    "print(cnt_tst_1)\n",
    "\n",
    "cnt_data_1=cnt_trn_1+cnt_tst_1\n",
    "print(cnt_data_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-stewart",
   "metadata": {},
   "source": [
    "#### Define the architecture of the neural network\n",
    "\n",
    "Initialise a sequential neural network model using keras.models.Sequential(), and add dense layers (dense layer means fully-connected layer) to it using keras.layers.Dense() (you easily how to do this from the internet).\n",
    "\n",
    "Use ReLU activation function in every layer, except the last one, where you'll use the sigmoid activation function, since it's a binary classification task.\n",
    "\n",
    "The choice of the number of layers and the number of units in a layer is totally up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fiscal-terror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 24)                168       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 1,993\n",
      "Trainable params: 1,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(24,activation=\"relu\",input_dim=6))\n",
    "model.add(keras.layers.Dense(24,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(24,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(24,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-drunk",
   "metadata": {},
   "source": [
    "#### Training the neural network\n",
    "\n",
    "1. Compile your nn model using model.compile() . Use the appropriate loss function (binary cross-entropy), and use Adam optimizer. Pass on 'accuracy' as a metric, so that you get to see the accuracy on your training set after every iteration of Adam optimization (a form of mini-batch gradient descent).\n",
    "\n",
    "Try to look-up and learn a bit about what stochastic gradient descent and mini-batch gradient descent essentilly are. You'll use mini-batches while training your model.\n",
    "\n",
    "2. Train your model using model.fit() (this will take a while, perhaps a few minutes). Use appropriate number of ephocs and batch size (you have to decide which values work the best). Don't forget that you have to train your model on the training set, and not the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "infrared-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "252/252 [==============================] - 1s 486us/step - loss: 0.0595 - accuracy: 0.9715\n",
      "Epoch 2/10\n",
      "252/252 [==============================] - 0s 494us/step - loss: 0.0597 - accuracy: 0.9729\n",
      "Epoch 3/10\n",
      "252/252 [==============================] - 0s 509us/step - loss: 0.0630 - accuracy: 0.9737\n",
      "Epoch 4/10\n",
      "252/252 [==============================] - 0s 505us/step - loss: 0.0590 - accuracy: 0.9735\n",
      "Epoch 5/10\n",
      "252/252 [==============================] - 0s 509us/step - loss: 0.0684 - accuracy: 0.9693\n",
      "Epoch 6/10\n",
      "252/252 [==============================] - 0s 521us/step - loss: 0.0597 - accuracy: 0.9739\n",
      "Epoch 7/10\n",
      "252/252 [==============================] - 0s 525us/step - loss: 0.0617 - accuracy: 0.9709\n",
      "Epoch 8/10\n",
      "252/252 [==============================] - 0s 505us/step - loss: 0.0637 - accuracy: 0.9717\n",
      "Epoch 9/10\n",
      "252/252 [==============================] - 0s 540us/step - loss: 0.0574 - accuracy: 0.9769\n",
      "Epoch 10/10\n",
      "252/252 [==============================] - 0s 638us/step - loss: 0.0561 - accuracy: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x200a0217e50>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#model.fit(normalized_trainingx,trainingy,batch_size=20,epochs=20)\n",
    "model.fit(A,trainingy,batch_size=20,epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-recording",
   "metadata": {},
   "source": [
    "#### Evaluation of the model on the test set\n",
    "\n",
    "Find the accuracy of your trained model on the test set. Don't forget that you had normalized your training set before training the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "stuck-samoa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.902433931557425e-14\n",
      "-2.220446049250313e-16\n",
      "3.241851231905457e-14\n",
      "3.5860203695392556e-14\n",
      "2.220446049250313e-15\n",
      "-3.1530333899354446e-14\n",
      "68/68 [==============================] - 0s 408us/step - loss: 19.3427 - accuracy: 0.9292\n",
      "68/68 [==============================] - 0s 379us/step - loss: 0.0930 - accuracy: 0.9509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09296112507581711, 0.9509259462356567]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_testx=normalize(testx)\n",
    "model.evaluate(normalized_testx,testy)\n",
    "model.evaluate(B,testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-mozambique",
   "metadata": {},
   "source": [
    "Is accuracy really the best metric to evaluate our model on the test set here? What was the proportion of positive samples to negative samples in the dataset? What would have been the accuracy of a model that would have output '0' for any input? By the way, this dataset is for detecting the presence of thyroid in a patient.\n",
    "\n",
    "Now, find out about the metrics - recall, precision and F1-score. Use these metrics to evaluate your model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "thirty-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 1s 481us/step - loss: 21.6446 - accuracy: 0.9229 - recall_8: 0.0671 - precision_8: 0.9446 - false_positives_8: 0.3188          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.34272575378418,\n",
       " 0.9291666746139526,\n",
       " 0.0731707289814949,\n",
       " 0.9230769276618958,\n",
       " 1.0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy',keras.metrics.Recall(),keras.metrics.Precision(),keras.metrics.FalsePositives()])\n",
    "model.evaluate(normalized_testx,testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "latin-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00007823 0.        ]\n",
      " [0.00004489 0.        ]\n",
      " [0.00127396 0.        ]\n",
      " ...\n",
      " [0.00018191 0.        ]\n",
      " [0.00428119 0.        ]\n",
      " [0.00002596 0.        ]]\n",
      "370 [0.71659338 0.88003665 0.77449715 0.71064514 0.70254529 0.84556508\n",
      " 0.8972463  0.80202854 0.58834904 0.77425492 0.4987042  0.77682287\n",
      " 0.7345956  0.73126698 0.78336656 0.73910701 0.71383089 0.83680898\n",
      " 0.70722312 0.75406581 0.7170257  0.66371644 0.72144681 0.92821634\n",
      " 0.7586832  0.78961849 0.4376848  0.73657507 0.76778138 0.74502933\n",
      " 0.886154   0.71391255 0.76817238 0.72353911 0.75715333 0.93165016\n",
      " 0.80823374 0.71600592 0.77538383 0.74017262 0.38074532 0.71741086\n",
      " 0.77806377 0.72059453 0.67094934 0.81574893 0.69438279 0.71744895\n",
      " 0.93949711 0.39259902 0.66595495 0.62877083 0.33821756 0.70668817\n",
      " 0.74008745 0.29173249 0.71173835 0.95075673 0.83967185 0.91347212\n",
      " 0.71579754 0.76874822 0.80826688 0.70863128 0.96969438 0.73130023\n",
      " 0.67474431 0.74539709 0.82342297 0.17112824 0.70678508 0.7949121\n",
      " 0.96523285 0.40943956 0.92968118 0.19566894 0.86018747 0.84168839\n",
      " 0.95859301 0.78001976 0.53553879 0.97788495 0.39247867 0.31530362\n",
      " 0.66479433 0.71301043 0.78232026 0.7116428  0.56550765 0.63952273\n",
      " 0.86740017 0.62415761 0.16403461 0.96488768 0.63779914 0.72047579\n",
      " 0.71967137 0.66592944 0.99963355 0.72874141 0.70918977 0.23714322\n",
      " 0.6399948  0.71405303 0.711658   0.86927491 0.25121307 0.9884181\n",
      " 0.7036413  0.76930463 0.71157879 0.77164447 0.87817657 0.8493073\n",
      " 0.72366357 0.38871878 0.72655106 0.70855445 0.72508526 0.7472946\n",
      " 0.67039448 0.9384253  0.70967489 0.97878325 0.72982818 0.65298975\n",
      " 0.66985905 0.7947247  0.84375197 0.74309933 0.99961793 0.71614242\n",
      " 0.93315053 0.71269727 0.72847199 0.71073341 0.8565855  0.72317809\n",
      " 0.47366142 0.77217257 0.71095693 0.73475444 0.72000909 0.77659857\n",
      " 0.53174549 0.74116325 0.50238174 0.86787963 0.72015107 0.50362891\n",
      " 0.76238596 0.98949492 0.79740798 0.80366051 0.14355302 0.38171113\n",
      " 0.96050984 0.63892192 0.72274327 0.71374148 0.99966586 0.71028447\n",
      " 0.83092713 0.50852209 0.42799637 0.83642578 0.71721166 0.86778033\n",
      " 0.77797437 0.72347468 0.89453423 0.86504745 0.83596015 0.81147826\n",
      " 0.61173964 0.97746146 0.70933843 0.71986073 0.70891929 0.72356606\n",
      " 0.99984288 0.38483509 0.7433551  0.71876615 0.69920599 0.73181635\n",
      " 0.69973063 0.97565991 0.19075218 0.9717561  0.8612656  0.69312763\n",
      " 0.80980533 0.57930213 0.19681549 0.67504561 0.71096277 0.96978807\n",
      " 0.78106666 0.83596003 0.99943101 0.62737417 0.18106559 0.68163097\n",
      " 0.71737003 0.69454694 0.71677482 0.70728922 0.57918864 0.99899685\n",
      " 0.93144536 0.70581889 0.4684343  0.76814175 0.48793933 0.71557558\n",
      " 0.71772027 0.72007978 0.70980555 0.34876108 0.65358824 0.91680425\n",
      " 0.32956445 0.98237652 0.65966958 0.59749115 0.71378326 0.93075001\n",
      " 0.76034224 0.76696122 0.74414706 0.68573642 0.60296273 0.71350539\n",
      " 0.73272741 0.80950963 0.78010082 0.4554441  0.71709394 0.75150692\n",
      " 0.70582139 0.73644465 0.77803636 0.87204492 0.96204317 0.71512693\n",
      " 0.70783556 0.82370925 0.71753681 0.70759618 0.71793681 0.21093777\n",
      " 0.82193959 0.78741449 0.73072892 0.70723736 0.71307111 0.68031949\n",
      " 0.90910017 0.83711517 0.46735477 0.77616704 0.94543564 0.76612675\n",
      " 0.9211812  0.57527226 0.82189167 0.83240247 0.26401949 0.83010519\n",
      " 0.80471253 0.70884168 0.73689067 0.87821329 0.71684909 0.7107079\n",
      " 0.71732402 0.71649313 0.85891414 0.7212218  0.45779148 0.71201044\n",
      " 0.31877819 0.72046578 0.19149718 0.87877214 0.62131619 0.77959192\n",
      " 0.74464041 0.80146885 0.71706724 0.78826332 0.78788179 0.94257444\n",
      " 0.44646627 0.7187745  0.85592091 0.71511495 0.7006892  0.33287591\n",
      " 0.98171198 0.98171437 0.92832458 0.71506917 0.95498765 0.75210476\n",
      " 0.66464031 0.91636038 0.46724588 0.787058   0.71627998 0.79032707\n",
      " 0.4194074  0.76664382 0.83751166 0.65647727 0.61288059 0.34185949\n",
      " 0.30627441 0.25067657 0.75627863 0.67853403 0.93694866 0.70076597\n",
      " 0.2089178  0.53270572 0.67012775 0.81286925 0.54674935 0.97396886\n",
      " 0.94317305 0.81098437 0.71339482 0.85345364 0.97067142 0.83336675\n",
      " 0.65745491 0.74299133 0.69931674 0.81300247 0.67913282 0.45613441\n",
      " 0.73391968 0.84015369 0.22453669 0.85387057 0.61863708 0.78229934\n",
      " 0.78245866 0.77667773 0.716748   0.96108687 0.60740185 0.75247526\n",
      " 0.24448806 0.71046656 0.78282875 0.47018436 0.72013235 0.72118896\n",
      " 0.71295261 0.25489533 0.65856838 0.75409496 0.80154872 0.17139184\n",
      " 0.73309457 0.81997061 0.71000177 0.1820997 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\htg_sensei\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "#predicted data\n",
    "prediction=np.hstack((model.predict(A),model.predict_classes(A)))\n",
    "np.set_printoptions(suppress=True)\n",
    "print(prediction)\n",
    "\n",
    "prediction_0=np.array([])\n",
    "prediction_1=np.array([])\n",
    "for i in range(len(prediction)):\n",
    "    if trainingy[i]==0:\n",
    "        prediction_0=np.append(prediction_0,prediction[i:i+1,:1])\n",
    "    else:\n",
    "        prediction_1=np.append(prediction_1,prediction[i:i+1,:1])\n",
    "print(len(prediction_1),prediction_1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-chapter",
   "metadata": {},
   "source": [
    "Now, tune the hyperparameters of your model (like number of layers, number of units in different layers, etc.) to try and do better and better on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-wound",
   "metadata": {},
   "source": [
    "\n",
    "#### Plotting histograms based on prediction values\n",
    "\n",
    "Now, you will plot 2 historgrams of prediction values (0 to 1, or 0% to 100%) on the training data, output by your nn model - one histogram for positive samples and another for negative samples. Plot both the histograms in the same figure. \n",
    "\n",
    "Here's one such plot I made with my nn model:\n",
    "<img src=\"files/index.jpeg\">\n",
    "\n",
    "The blue histogram is for negative samples and the red one is for positive samples (add a legend to the plots, unlike me).\n",
    "\n",
    "Use log-scale on the y-axis (number of occurances in the given predicted value range), like I have done.\n",
    "\n",
    "Now, what can you infer from the histograms you got? How should an ideal pair of histograms look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dated-viewer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the histogram\n",
    "#plt.hist(prediction_1,bins=np.arange(0,1,0.05))\n",
    "plt.hist([prediction_0,prediction_1],bins=np.arange(0,1,0.05),color=['b','r'],label=['Negative','Positive'],rwidth=1)\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Occurances')\n",
    "plt.yscale('log') # setting yscale\n",
    "plt.title('Prediction Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-darkness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-beijing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-heading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-trance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-pleasure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
